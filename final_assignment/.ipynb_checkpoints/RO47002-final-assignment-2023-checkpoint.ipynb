{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5955546",
   "metadata": {},
   "source": [
    "# Final Assignment RO47002 - Machine Learning for Robotics 2023/2024\n",
    "\n",
    "Before you start, fill in the cell below your lab group's number and the names of both lab partners.\n",
    "It is also suggested that you carefully read through all provided content before you start adding things.\n",
    "\n",
    "*Note*: as always, basic plagiarism and ethical guidelines apply:\n",
    "* By submitting this notebook, **you both claim that the solution is yours and yours only.**\n",
    "* You are not allowed to share your work with others.\n",
    "* Even after the deadline has passed, do *not* share or upload your solution anywhere (e.g. do not put it on github)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b3ab9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_NUMBER = \"74\"\n",
    "STUDENT_NAME1 = \"Kevin Tran\"\n",
    "STUDENT_NUMBER1 = \"4904672\"\n",
    "STUDENT_NAME2 = \"Dielof van Loon\"\n",
    "STUDENT_NUMBER2 = \"5346894\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d8f97ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this block is a check that you have filled in the above information.\n",
    "# It will throw an AssertionError until all fields are filled\n",
    "assert(GROUP_NUMBER != \"\")\n",
    "assert(STUDENT_NAME1 != \"\")\n",
    "assert(STUDENT_NUMBER1 != \"\")\n",
    "assert(STUDENT_NAME2 != \"\")\n",
    "assert(STUDENT_NUMBER2 != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a819d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction\n",
    "We are in the year 2023. Tech companies are striving to develop robotic helpers to help people with their day to day tasks. Your employer is working on a new feature for their robots. The feature aims at enabling their robots to go outside and collect deliveries for their owners. The deliveries would be dropped off at designated locations in front of a person's home. The robots should be able to navigate safely towards the delivered parcels, taking care not to enter the road. They should also be able to operate at any given time of day.\n",
    "\n",
    "For the upcoming demo to their stakeholders, your employer would like to showcase the robot going along the sidewalk to collect a red parcel. The demo is to take place within a blocked off part of the sidewalk in front of your workplace. Safety cones have been propped up next to the sidewalk in order to have a safety bound between the sidewalk and the road. The robot needs to remain on the sidewalk and go to the parcel at the end of the sidewalk. To prepare for the demo, your employer has booked three timeslots with the robot throughout the week. These timeslots ('Time 0', 'Time 1', and 'Time 2') are at different times of the day.\n",
    "\n",
    "As you are just starting out as a robot AI engineer, you first get the familiarization assignment to train a model solely for opperating at 'Time 1' before moving on to designing a more general AI.\n",
    "\n",
    "## Robot AI\n",
    "\n",
    "In this task, the robot's actions need to be determined based on only the robot's last observation (i.e. sensor measurements). More technically: the goal is to create a function $policy(observation) \\rightarrow action$, which the robot can continuously apply in a loop on its sensor measurements to determine its next action. This type of function is often called a *policy*.\n",
    "\n",
    "In this assignment, the input and output of the policy function `policy` are defined as:\n",
    "- the input `observation` will be an RGB image (a numpy array) containing a view of the robot's surroundings.\n",
    "- the output `action` should be an integer out of one of the possible actions (an integer between 0 and 2):\n",
    "\n",
    "    0. Turn left\n",
    "    1. Turn right\n",
    "    2. Move forward\n",
    "\n",
    "[comment]:<> (More details will be provided later in the section 0. Code to get you started\" below.)\n",
    "\n",
    "Note that the policy function will completely determine the behavior of the robot.\n",
    "You can think of the robot executing the policy in a never-ending loop (in pseudo-code):\n",
    "```python\n",
    "# Pseudo robot main loop with policy f\n",
    "while True:\n",
    "    observation = read_sensor_measurement()\n",
    "    action = policy(observation) # apply the policy\n",
    "    execute_action(action)\n",
    "```\n",
    "In fact, the behavior is already implemented for you in a **simulator**. This means you can test a policy function `policy` by plugging it in the simulator and seeing how your robot behaves!\n",
    "The simulator also allows us to quantify how well your robot behaved by returning a 'reward' value for each simulation step (a higher reward is better). More details on the rewards will be explained below in Section 0.\n",
    "\n",
    "## Task description: imitation learning\n",
    "\n",
    "These type of tasks, where a robot's policy needs to be optimized to achieve a high expected reward for operating in some environment, is often addressed through *reinforcement learning*. However, in this assignment we will *not* use reinforcement learning techniques, but mainly treat the problem as a **supervised classification task**.\n",
    "The training input (observations) and output (action labels) data will be obtained through demonstrations of humans *manually* controlling the robot. By training your machine learning models on these demonstrations, you will create an AI which \"imitates\" how a human would control the robot. This type of supervised machine learning is therefore called *imitation learning*.\n",
    "\n",
    "There are some coding challenges and design choices that you have to solve to use human demonstrations as labeled data to create your machine learning models. As before, you will have to think about feature extraction, hyperparemeter optimization, evaluation metrics, comparing models, etc.\n",
    "\n",
    "Once you have trained a classification model, you could define a new policy function which uses your trained model, and test this policy in the simulator. Does your most successful model also accumulate the most reward in the simulation? Can you make an AI which successfully operates at recorded and unrecorded times of the day?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef10e769",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "The deadline is **Tuesday, October 31th, 2023 at 12:00**. Late submission is â€“1 grade point per day.\n",
    "\n",
    "* The main deliverable is this Jupyter Notebook, integrating the report (markdown cells) and the code.\n",
    "* Submission is again in the form of a single ZIP file that *includes your notebook, and all files required to run the notebook and reproduce the results*. This includes all used data/demonstrations (including the ones that were provided), any loadable parameter files, any auxiliary scripts, etc.\n",
    "* Name the ZIP file \"**GroupNumber_final_assignment.zip**\", e.g., if you are in group 456, the name would be \"456_final_assignment.zip\".\n",
    "* Unlike previous lab assignments, there are no autograded cells or asserts, but we will grade the notebook manually. Therefore, you are free to add cells as you see fit, as long as the required sections are still present in the notebook.\n",
    "* Make sure that the notebook runs correctly. That is, clear all outputs, restart the kernel and run the notebook from top to bottom. \n",
    "* The notebook needs to be able to run within 20 minutes on a high-end PC, performing all steps (also including training, the only exception is hyperparameter optimization which can be commented out).\n",
    "* In contrast to the practica, please submit the notebook *including* the output (i.e., do not clear the outputs before zipping it up). \n",
    "\n",
    "\n",
    "## Grading Criteria\n",
    "Below you will find an outline of the sections that the notebook needs to contain and what we expect for each part. More specific requirements are listed there as well. The indicated number of points, out of a total of 100, should give you a rough indication of how much effort to put into each part.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In general, we will not focus as much on the performance of the method you design, but rather the <b>level of understanding and argumentation about your design choices</b>. So, we are not only interested in WHAT you did, but will put a strong emphasis on your reasoning about the WHY. Try to synthesize rather than describing what you did step by step.\n",
    "</div>\n",
    "\n",
    "### Quality of the report (20 points)\n",
    "- Structure & Readability\n",
    " - Logical flow\n",
    " - Connection between parts\n",
    "- Academic English\n",
    " - Do not use short forms, so not \"isn't\" / \"wouldn't\" but \"is not\" / \"would not\".\n",
    " - Do not use colloquial style, like \"a couple of\".\n",
    " - Spell check and proofread your report.\n",
    "- Level of detail\n",
    " - Strive for elegant, concise text - longer reports do not necessarily yield higher grades.\n",
    " - There is no need to re-explain theory. Assume that the target audience of the report has followed the course.\n",
    "- Figures & Tables\n",
    " - Choose figures/plots/tables carefully. Only include those that add to the story of the report. Do not put the burden on the reviewer to figure out which results you are basing your conclusions on, but specifically refer (parts of) the specific table/plot/figure when needed.\n",
    " - When comparing two or more signals display them in one plot. Explain the colors / line types. The scale of the plots must be carefully chosen in order to clearly convey the information intended. Label the axes in graphs properly (variables and units).\n",
    "- Citations\n",
    " - If you use images, theory and methods beyond what was covered in the course, etc., always reference sources.\n",
    "\n",
    "\n",
    "### Your implementations and answers (80 points)\n",
    "\n",
    "The remainder of this notebook follows the following structure:\n",
    "\n",
    "0. Code to get you started (0 points, *nothing for you to do here*)\n",
    "1. Explore & Inspect the Data (5 points)\n",
    "2. Prepare the Data and Evaluate Features (15 points)\n",
    "3. Single Time of Day Action Classification  (35 points)\n",
    "4. Enabling Generalization (20 points)\n",
    "5. Present your solution (5 points)\n",
    "\n",
    "Apart from section 0, you will have to implement and answer questions for all of the other 5 sections to earn points. *For each of these 5 sections, we have various questions or implementation tasks that your submission should address. These are listed in the cells at the end of this notebook.*\n",
    "\n",
    "Note that there is not one best answer to these questions, and the task could be addressed in different ways. We want to know *your* motivation for *your* selected approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf01613",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 0. Code to get you started\n",
    "\n",
    "Before you start working on this notebook, **you will first need to install the Miniworld environment in your `homl3` conda environment**\n",
    "```\n",
    "pip install miniworld\n",
    "```\n",
    "\n",
    "**You will also need to install OpenCV in your `homl3` conda environment**\n",
    "```\n",
    "pip install opencv-python\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "If you encounter \"zmq message arrived on closed channel\" errors when working on this notebook, ensure that you do not have AdBlockers, Firewalls or similar active. If this does not resolve the issue, contact us regarding your particular case.\n",
    "</div>\n",
    "\n",
    "Note, you will not have to implement anything in this section, but you are free to play around with what is provided here, or copy parts to new cells in your solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d7ea8d",
   "metadata": {},
   "source": [
    "## Global flags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786bd5f6",
   "metadata": {},
   "source": [
    "Here we define two global flags that will influence if this notebook will show popup windows when running the simulation:\n",
    "- `show_policy` defines whether you will see a pop-up window visualizing the policy from a random initial configuration\n",
    "- `collect_demo` defines whether you will see an interactive window that allows you to collect your own demonstrations\n",
    "\n",
    "Feel free to change the flags later if you need. For instance, if you want to test if your notebook runs properly and reproduces your results, but you do not want to wait for the popup windows, you can set these flags to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a5a7c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure if the simulator should show pop-up windows\n",
    "# CHANGE IF NEEDED\n",
    "show_policy = True\n",
    "collect_demo = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733da8fe",
   "metadata": {},
   "source": [
    "## Load default modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "192dd7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2 # you are allowed to use functions from cv2\n",
    "import glob\n",
    "\n",
    "# import the simulation environment\n",
    "import sidewalk_ro47002\n",
    "from sidewalk_ro47002 import Sidewalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5a81daf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (this cell is included for debugging, it makes it easy to reload sidewalk_ro47002.py without restarting the kernel)\n",
    "import importlib\n",
    "importlib.reload(sidewalk_ro47002)\n",
    "from sidewalk_ro47002 import Sidewalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a1fbe749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating environments for the 4 environments with rendering\n",
    "envs = []\n",
    " \n",
    "for n in range(4): \n",
    "    envs.append(Sidewalk(time_id=n, max_episode_time_step=1000, rendering=True))\n",
    "    \n",
    "# N.B.: on some computer you might get a warning \"Cannot change thread mode after it is set\"\n",
    "# when you run this cell the first time. It is safe to ignore this warning, the notebook should still work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8625ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating environments for the 4 environments without rendering\n",
    "envs_norender = []\n",
    " \n",
    "for n in range(4): \n",
    "    envs_norender.append(Sidewalk(time_id=n, max_episode_time_step=1000, rendering=False)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1127d2d",
   "metadata": {},
   "source": [
    "First, we explore the available environments. The code below generates an image from the 3 environments. You do not need to understand how this code works, but it should help you understand the context of what we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d2a595ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267957931b564c8dba6e63ac524286aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='time_id', max=2), Output()), _dom_classes=('widget-interâ€¦"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets\n",
    "import pyglet\n",
    "\n",
    "# Show screenshot of a sampled environment\n",
    "def plot_time_example(time_id):\n",
    "    env = envs_norender[time_id]\n",
    "    env.reset(seed=1)\n",
    "    plt.imshow(env.render())\n",
    "    env.render()\n",
    "    plt.title(f'Time {time_id}')\n",
    "    \n",
    "\n",
    "ipywidgets.interactive(plot_time_example, time_id=(0,2), continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed13a94",
   "metadata": {},
   "source": [
    "## Loading pre-recorded human demonstrations\n",
    "\n",
    "You are provided several pre-recorded demonstrations of a human *manually controlling* the robot on several runs in several environments.\n",
    "**You can use this data to train a classifier that you can use to implement one or more better policies, which should (ideally) perform similar to how a human would control the robot.**\n",
    "\n",
    "Each provided demonstration ...\n",
    "* ... contains a sequence of (observation, action) pairs ...\n",
    "* ... recorded at a specific time of day, ...\n",
    "* ... for convenience, also contains arrays containing the fixed time id of these input/output pair.\n",
    "\n",
    "A demonstration is stored as a python pickle file.\n",
    "The code below shows how to load the saved demonstrations, and to do some simple pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e11bb94b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76 demonstrations\n",
      "Loaded 62 samples from demonstrations\\demo-0-20230929_113424.pickle ...\n",
      "Loaded 3 samples from demonstrations\\demo-0-20230929_125159.pickle ...\n",
      "Loaded 12 samples from demonstrations\\demo-0-20230929_125237.pickle ...\n",
      "Loaded 7 samples from demonstrations\\demo-0-20230929_125255.pickle ...\n",
      "Loaded 22 samples from demonstrations\\demo-0-20230929_125307.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-0-20230929_125335.pickle ...\n",
      "Loaded 1 samples from demonstrations\\demo-0-20230929_125419.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-0-20230929_125518.pickle ...\n",
      "Loaded 3 samples from demonstrations\\demo-0-20230929_125612.pickle ...\n",
      "Loaded 5 samples from demonstrations\\demo-0-20230929_125639.pickle ...\n",
      "Loaded 18 samples from demonstrations\\demo-0-20230929_125659.pickle ...\n",
      "Loaded 6 samples from demonstrations\\demo-0-20230929_162031.pickle ...\n",
      "Loaded 4 samples from demonstrations\\demo-0-20230929_162115.pickle ...\n",
      "Loaded 7 samples from demonstrations\\demo-0-20230929_162258.pickle ...\n",
      "Loaded 5 samples from demonstrations\\demo-0-20230929_162340.pickle ...\n",
      "Loaded 7 samples from demonstrations\\demo-0-20230929_162352.pickle ...\n",
      "Loaded 1 samples from demonstrations\\demo-0-20231003_094248.pickle ...\n",
      "Loaded 1 samples from demonstrations\\demo-0-20231003_094259.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-0-20231003_094325.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-0-20231003_094549.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-0-20231003_094734.pickle ...\n",
      "Loaded 4 samples from demonstrations\\demo-0-20231003_094743.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-0-20231003_094750.pickle ...\n",
      "Loaded 70 samples from demonstrations\\demo-1-20230929_171322.pickle ...\n",
      "Loaded 6 samples from demonstrations\\demo-1-20230929_171459.pickle ...\n",
      "Loaded 3 samples from demonstrations\\demo-1-20230929_171509.pickle ...\n",
      "Loaded 1 samples from demonstrations\\demo-1-20230929_171533.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-1-20230929_171553.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-1-20230929_171609.pickle ...\n",
      "Loaded 22 samples from demonstrations\\demo-1-20230929_171716.pickle ...\n",
      "Loaded 3 samples from demonstrations\\demo-1-20230929_171751.pickle ...\n",
      "Loaded 12 samples from demonstrations\\demo-1-20230929_171804.pickle ...\n",
      "Loaded 6 samples from demonstrations\\demo-1-20230929_171833.pickle ...\n",
      "Loaded 3 samples from demonstrations\\demo-1-20230929_171844.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-1-20230929_171922.pickle ...\n",
      "Loaded 8 samples from demonstrations\\demo-1-20230929_172005.pickle ...\n",
      "Loaded 16 samples from demonstrations\\demo-1-20230929_172027.pickle ...\n",
      "Loaded 3 samples from demonstrations\\demo-1-20230929_172043.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-1-20230929_172111.pickle ...\n",
      "Loaded 1 samples from demonstrations\\demo-1-20230929_172127.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-1-20230929_172151.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-1-20230929_172226.pickle ...\n",
      "Loaded 1 samples from demonstrations\\demo-1-20230929_172258.pickle ...\n",
      "Loaded 6 samples from demonstrations\\demo-1-20230929_172317.pickle ...\n",
      "Loaded 3 samples from demonstrations\\demo-1-20230929_172328.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-1-20230929_172458.pickle ...\n",
      "Loaded 4 samples from demonstrations\\demo-1-20230929_172525.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-1-20231003_095243.pickle ...\n",
      "Loaded 1 samples from demonstrations\\demo-1-20231003_095343.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-1-20231003_095653.pickle ...\n",
      "Loaded 4 samples from demonstrations\\demo-1-20231003_100023.pickle ...\n",
      "Loaded 68 samples from demonstrations\\demo-2-20231003_113230.pickle ...\n",
      "Loaded 7 samples from demonstrations\\demo-2-20231003_113256.pickle ...\n",
      "Loaded 7 samples from demonstrations\\demo-2-20231003_113543.pickle ...\n",
      "Loaded 1 samples from demonstrations\\demo-2-20231003_114355.pickle ...\n",
      "Loaded 1 samples from demonstrations\\demo-2-20231003_114607.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-2-20231003_114622.pickle ...\n",
      "Loaded 1 samples from demonstrations\\demo-2-20231003_114828.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-2-20231003_115053.pickle ...\n",
      "Loaded 3 samples from demonstrations\\demo-2-20231003_115133.pickle ...\n",
      "Loaded 3 samples from demonstrations\\demo-2-20231003_115712.pickle ...\n",
      "Loaded 4 samples from demonstrations\\demo-2-20231003_115726.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-2-20231003_115818.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-2-20231003_115945.pickle ...\n",
      "Loaded 3 samples from demonstrations\\demo-2-20231003_120007.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-2-20231003_120058.pickle ...\n",
      "Loaded 6 samples from demonstrations\\demo-2-20231003_120154.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-2-20231003_120239.pickle ...\n",
      "Loaded 4 samples from demonstrations\\demo-2-20231003_120305.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-2-20231003_120321.pickle ...\n",
      "Loaded 3 samples from demonstrations\\demo-2-20231003_120347.pickle ...\n",
      "Loaded 15 samples from demonstrations\\demo-2-20231003_120429.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-2-20231003_120503.pickle ...\n",
      "Loaded 3 samples from demonstrations\\demo-2-20231003_121328.pickle ...\n",
      "Loaded 4 samples from demonstrations\\demo-2-20231003_121350.pickle ...\n",
      "Loaded 2 samples from demonstrations\\demo-2-20231003_121654.pickle ...\n",
      "522 samples loaded\n"
     ]
    }
   ],
   "source": [
    "# Look for all the demonstration pickle files in the demonstrations/ directory.\n",
    "#  The originally provided demo files are called: demo-[time_id]-[datetime].pickle\n",
    "#  Any demo files you record yourself are called: demostud-[time_id]-[datetime].pickle\n",
    "\n",
    "# CHANGE THIS IF NEEDED: select the pickle file pattern to match ...\n",
    "\n",
    "DEMO_FILEPATTERN = 'demo-*-*.pickle'      # only use ORIGINALLY provided demo files\n",
    "#DEMO_FILEPATTERN = 'demostud-*-*.pickle'  # only use YOUR own collected demo files\n",
    "#DEMO_FILEPATTERN = 'demo*-*.pickle'        # use ALL available demo files\n",
    "\n",
    "# find the relevant filenames\n",
    "filenames = glob.glob(f'demonstrations/{DEMO_FILEPATTERN}')\n",
    "filenames.sort() # ensure the order is well-defined\n",
    "print(f'Found {len(filenames)} demonstrations')\n",
    "\n",
    "# in a loop, load the found pickle files\n",
    "demonstrations = []\n",
    "total_samples = 0\n",
    "for filename in filenames:\n",
    "    with open(filename, 'rb') as fd:\n",
    "        demonstration = pickle.load(fd)\n",
    "        \n",
    "        actions = demonstration['actions']\n",
    "        print(f'Loaded {actions.shape[0]} samples from {filename} ...')\n",
    "        total_samples += actions.shape[0]\n",
    "        demonstrations.append(demonstration)\n",
    "print(f\"{total_samples} samples loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "388fd5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can combine all the observations, actions, and time ids\n",
    "\n",
    "observations = np.concatenate([d['observations'] for d in demonstrations])\n",
    "actions = np.concatenate([d['actions'] for d in demonstrations])\n",
    "time_ids = np.concatenate([d['time'] for d in demonstrations])\n",
    "\n",
    "# pre-processing: subsample and only keep every n-th sample for efficiency\n",
    "# this can speed up training\n",
    "ss = 1\n",
    "observations = observations[::ss]\n",
    "actions = actions[::ss]\n",
    "time_ids = time_ids[::ss]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c823a",
   "metadata": {},
   "source": [
    "## Understanding the data\n",
    "\n",
    "We here take a closer look at format of the demonstration data. The observations (input) are RGB images. The actions (target class labels) are integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "656bccb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data consists of 522 (observation, action) pairs:\n",
      "- observations : a (522, 60, 80, 3) numpy int8 array, i-th entry contains RGB image of sample i\n",
      "- actions      : a (522,) numpy int array, i-th entry contains action (class) label of sample i\n",
      "- time_ids     : a (522,) numpy int array, i-th entry contains the time_id of sample i\n"
     ]
    }
   ],
   "source": [
    "# count the total number of observations\n",
    "N = observations.shape[0]\n",
    "\n",
    "print(f'The data consists of {N} (observation, action) pairs:')\n",
    "print(f'- observations : a {observations.shape} numpy int8 array, i-th entry contains RGB image of sample i')\n",
    "print(f'- actions      : a {actions.shape} numpy int array, i-th entry contains action (class) label of sample i')\n",
    "print(f'- time_ids     : a {time_ids.shape} numpy int array, i-th entry contains the time_id of sample i')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f121026e",
   "metadata": {},
   "source": [
    "We can inspect inspect a single sample in the recorded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "db3a2b2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action of sample 20:                     2\n",
      "Time id when sample 20 was recorded:     0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 20 # sample index, should be in range [0, N-1]\n",
    "\n",
    "print(f'Action of sample {idx}:                    ', actions[idx])\n",
    "print(f'Time id when sample {idx} was recorded:    ', time_ids[idx])\n",
    "\n",
    "plt.imshow(observations[idx])\n",
    "plt.title(f'Observation of sample {idx}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff18b10",
   "metadata": {},
   "source": [
    "Note that the observation is just a low-resolution image of the simulated environment.\n",
    "\n",
    "An action (label) is simply an integer. The three possible action values are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3021679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_LEFT = 0  # Turn left\n",
    "ACTION_RIGHT = 1 # Turn right\n",
    "ACTION_FORWARD = 2  # Move forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82908e5a",
   "metadata": {},
   "source": [
    "You can use the demonstration data to train and validate your machine learning methods.\n",
    "Of course, you would first need to define some feature extraction procedure(s) to convert the observations into some suitable feature vectors for your machine learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a476e18",
   "metadata": {},
   "source": [
    "### Replay demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b15c060",
   "metadata": {},
   "source": [
    "The following cell allows you to replay some of the demonstrations found in the dataset.\n",
    "\n",
    "Study the demonstrations. \n",
    "\n",
    " - Are you able to recognize a specific strategy employed by the human demonstrator to solve this environment? \n",
    " - What would happen if different strategies were employed by the demonstrator? How does this relate to *irreducible error*?\n",
    "\n",
    "If you were to collect demonstrations yourself, these are questions that you should take into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "536b6ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b4353f51c5494d8fbdc351b898efa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='demo_id', max=2), Output()), _dom_classes=('widget-interâ€¦"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import animation\n",
    "from IPython.display import HTML, display, clear_output\n",
    "import ipywidgets\n",
    "\n",
    "\n",
    "class ReplayDemos(animation.FuncAnimation):\n",
    "    def __init__(self, demo_id):\n",
    "        # np array with shape (frames, height, width, channels)\n",
    "        self.video_init = np.array(complete_demonstrations[demo_id]['observations'])\n",
    "        \n",
    "        # Add black frames at the end of the video\n",
    "        length_end = 20\n",
    "        self.end_video = np.zeros(np.append(length_end, self.video_init.shape[1:]), dtype=np.int)\n",
    "        self.video = np.append(self.video_init, self.end_video, axis=0)\n",
    "        \n",
    "        # Create figure\n",
    "        self.fig = plt.figure()\n",
    "        self.im = plt.imshow(self.video[0,:,:,:])\n",
    "        \n",
    "        plt.title('Replaying demonstration %i' % demo_id)\n",
    "        plt.close() # this is required to not display the generated image\n",
    "        \n",
    "        # Init parent\n",
    "        super().__init__(self.fig, self.animate, init_func=self.init, frames=self.video.shape[0], interval=50)\n",
    "        \n",
    "    def init(self):\n",
    "        self.im.set_data(self.video[0,:,:,:])\n",
    "\n",
    "    def animate(self, i):\n",
    "        self.im.set_data(self.video[i,:,:,:])\n",
    "        return self.im\n",
    "\n",
    "\n",
    "def filter_demos(demonstrations, thr=50):\n",
    "    # Remove all short demonstrations\n",
    "    complete_demonstrations = []\n",
    "    for demo in demonstrations:\n",
    "        if demo['observations'].shape[0] > thr:\n",
    "            complete_demonstrations.append(demo)\n",
    "            \n",
    "    return complete_demonstrations\n",
    "\n",
    "complete_demonstrations = filter_demos(demonstrations, thr=50)\n",
    "\n",
    "def replay_function(demo_id):\n",
    "    clear_output(wait=True)\n",
    "    replay_demos = ReplayDemos(demo_id)\n",
    "    display(HTML(replay_demos.to_html5_video()))\n",
    "\n",
    "ipywidgets.interactive(replay_function, demo_id=(0,len(complete_demonstrations)-1), continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb2415",
   "metadata": {},
   "source": [
    "## Testing your model in  the simulator\n",
    "\n",
    "\n",
    "At some point, *after you have trained and evaluated your classifier*, you might want to check how well your classification method can actually control the robot. For this, you will need to wrap your trained classifier into a policy function that the simulator can use.\n",
    "\n",
    "This section will go over the details of\n",
    "* how to start the simulator;\n",
    "* how to implement a policy `policy`;\n",
    "* how to analyse the rewards.\n",
    "\n",
    "\n",
    "### Running a simulation\n",
    "\n",
    "The function `run_simulation` below will setup a simulation of your robot on a given environment (`env`) and will use the policy `policy` you provide to control the robot, for a maximum of `max_steps` simulation steps. The previously created environments `env = Sidewalk(time_id, max_episode_time_step=1000, rendering=False)` determine the environment and whether the simulation is shown in a popup-window. The simulation function can also record all the (observation, actions) pairs (`record_data=True`).\n",
    "\n",
    "The function signature of `run_simulation` is:\n",
    "```python\n",
    "rewards = run_simulation(policy, env, max_steps=500, verbose=1, record_data=False, delay=0.0, seed=-1, human_control=False)\n",
    "    \"\"\" Run robot simulation\n",
    "    Input arguments:\n",
    "    - policy        # [function] the robot's policy function\n",
    "    - env           # [instance of Sidewalk] the environment to simulate\n",
    "    - record_data   # [True/False] if true, return all (observation, action) pairs from the simulation\n",
    "    - max_steps     # [int] the maximum number of steps N to run the simulation\n",
    "    - delay         # [float] a time delay that can be added to make the simulation run a bit slower\n",
    "    - seed          # [int] location generation random seed - only set if >=0\n",
    "    - verbose       # [int] how much text gets printed: 0 = none, 1 = final stats, 2 = all\n",
    "    - human_control # [True/False] if true the key presses will be passed on the policy\n",
    "    \n",
    "    Returns:\n",
    "    - rewards       # [numpy array of floats] all N rewards accumulated during the simulation\n",
    "    - observations  # [numpy array N x H x W x 3] N observations, each observation being a WxH 3-channel image\n",
    "    - actions       # [numpy array of ints] all N actions outputted by the given policy f\n",
    "    \n",
    "    Note: `observations` and `actions` are only returned if record_data=True\n",
    "    \"\"\" \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "23f56f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user key input handler\n",
    "from pyglet.window import key\n",
    "import time\n",
    "\n",
    "# keep track if which keys have been pressed in the popup window\n",
    "# (will be used later for the human control)\n",
    "KEY_PRESSED = {key.LEFT: False, key.RIGHT: False, key.UP: False}\n",
    "\n",
    "# define the set of all actions    \n",
    "ACTIONS = [0, 1, 2]\n",
    "ACTION_NAMES = ['turn_left', 'turn_right', 'move_forward']\n",
    "NUM_ACTIONS = 3 # number of distinct actions\n",
    "\n",
    "def run_simulation(policy, env, max_steps=500, verbose=1, record_data=False, delay=0.0, seed=-1, human_control=False):\n",
    "    \"\"\" Run robot simulation\n",
    "    Input arguments:\n",
    "    - policy        # [function] the robot's policy function\n",
    "    - env           # [instance of Sidewalk] the environment to simulate\n",
    "    - record_data   # [True/False] if true, return all (observation, action) pairs from the simulation\n",
    "    - max_steps     # [int] the maximum number of steps N to run the simulation\n",
    "    - delay         # [float] a time delay that can be added to make the simulation run a bit slower\n",
    "    - seed          # [int] location generation random seed - only set if >=0\n",
    "    - verbose       # [int] how much texts gets printed: 0 = none, 1 = final stats, 2 = all\n",
    "    - human_control # [True/False] if true the key presses will be passed on the policy\n",
    "    \n",
    "    Returns:\n",
    "    - rewards       # [numpy array of floats] all N rewards accumulated during the simulation\n",
    "    - observations  # [numpy array N x H x W x 3] N observations, each observation being a WxH 3-channel image\n",
    "    - actions       # [numpy array of ints] all N actions outputted by the given policy f\n",
    "    \n",
    "    Note: `observations` and `actions` are only returned if record_data=True\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print(f'Starting simulation for {max_steps} steps.')\n",
    "        if env.rendering:\n",
    "            print('*** Press ESC key in popup window to stop the simulation! ***')\n",
    "        print()\n",
    "\n",
    "    if seed>=0:\n",
    "        env.reset(seed=seed)\n",
    "    else:\n",
    "        env.reset(seed=int( time.time() * 1000.0 ))\n",
    "\n",
    "    rewards = [] # will store the accumulated rewards\n",
    "    observations = [] # will store the accumulated observations (only if record_data==True)\n",
    "    actions = [] # will store the accumulated actions outputted by policy f (only if record_data==True)\n",
    "\n",
    "    completed_steps = 0   \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # reset the simulation, and get the initial observation (robot \"sensor measurement\")\n",
    "        obs = env.reinit(max_steps, wait_for_keypress=human_control)[0]\n",
    "        \n",
    "        # main simulation loop\n",
    "        for step in range(max_steps):\n",
    "            time.sleep(delay)\n",
    "               \n",
    "            if env.rendering:\n",
    "                env.render()\n",
    "            \n",
    "            # get keyboard pressed button status from environment\n",
    "            KEY_PRESSED.update(env.key_pressed)\n",
    "\n",
    "            if env.stop_simulation: break\n",
    "            \n",
    "                \n",
    "            # ** APPLYING YOUR POLICY **\n",
    "            # execute the given policy on the observation to determine the robot's action\n",
    "            action = policy(obs)\n",
    "            \n",
    "            # sanity check: is the policy implemented correctly?\n",
    "            assert (isinstance(action, (int, np.integer))) # returned action should be a builtin or numpy integer\n",
    "            assert (action in ACTIONS) # action should be an integer 0, 1, 2 or 3\n",
    "            \n",
    "            if verbose > 1:\n",
    "                print(f'step {step}: action = {ACTION_NAMES[action]}')\n",
    "\n",
    "            if record_data:\n",
    "                # only store all the observation and action pairs during the simulation\n",
    "                #   if the record_data argument is set to True\n",
    "                observations.append(obs)\n",
    "                actions.append(action)\n",
    "\n",
    "            # execute simulation step with the given control input\n",
    "            obs, reward, environment_done, _, info = env.step(action)\n",
    "            completed_steps += 1\n",
    "            \n",
    "            if verbose > 1:\n",
    "                print(f'step {step}: reward = {reward}')\n",
    "\n",
    "            # collect all rewards in a list\n",
    "            rewards.append(reward)\n",
    "\n",
    "            # exit simulation when goal is reached\n",
    "            if environment_done:\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        # close the pop-up window,\n",
    "        if env.rendering:\n",
    "            env.close()\n",
    "            env.window = None\n",
    "        \n",
    "    rewards = np.array(rewards)\n",
    "    total_reward = np.sum(rewards)\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print(f'total reward after {completed_steps} steps: {total_reward}')\n",
    "        print(f'average reward: {total_reward/completed_steps}')\n",
    "    \n",
    "    if record_data:\n",
    "        return rewards, np.array(observations), np.array(actions, dtype=int)\n",
    "    \n",
    "    # by default, only return the rewards\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce82c5e",
   "metadata": {},
   "source": [
    "The function `test_policy` below will run the `policy` in given environment (`env`) for a number `no_runs` different initial configurations. This function ensures that always the same set of configurations is used, allowing you to reproducibly compare the performance of different policies.\n",
    "\n",
    "The function signature of `test_policy` is:\n",
    "```python\n",
    "no_completed, avg_reward = test_policy(policy, env, verbose=0, no_runs=10)\n",
    "    \"\"\" Test a policy\n",
    "    Input arguments:\n",
    "    - policy        # [function] the robot's policy function\n",
    "    - env           # [instance of Sidewalk] the environment to simulate\n",
    "    - verbose       # [int] how much text gets printed: 0 = none, 1 = final stats, 2 = all\n",
    "    - no_runs       # [int] how many configurations to evaluate\n",
    "    \n",
    "    Returns:\n",
    "    - no_completed  # [int] number of runs in which the goal was reached\n",
    "    - avg_reward    # [float] average reward over the runs\n",
    "    \"\"\" \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e8d80d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(policy, env, verbose=0, no_runs=10):\n",
    "    \"\"\" Test a policy\n",
    "    Input arguments:\n",
    "    - policy        # [function] the robot's policy function\n",
    "    - env           # [instance of Sidewalk] the environment to simulate\n",
    "    - verbose       # [int] how much text gets printed: 0 = none, 1 = final stats, 2 = all\n",
    "    - no_runs       # [int] how many configurations to evaluate\n",
    "    \n",
    "    Returns:\n",
    "    - no_completed  # [int] number of runs in which the goal was reached\n",
    "    - avg_reward    # [float] average reward over the runs\n",
    "    \"\"\" \n",
    "    \n",
    "    no_completed = 0\n",
    "    avg_reward = 0.0\n",
    "  \n",
    "    seeds = range(0,no_runs)\n",
    "    run_count = 1\n",
    "    for seed in seeds:\n",
    "        print('Run %i / %i' % (run_count, no_runs))\n",
    "        success = False\n",
    "        run_count += 1\n",
    "        rs = run_simulation(\n",
    "            policy,\n",
    "            env,\n",
    "            record_data=False,\n",
    "            verbose=verbose, \n",
    "            max_steps=300, \n",
    "            delay=0.0,\n",
    "            seed=seed)\n",
    "\n",
    "        avg_reward += np.mean(rs)\n",
    "        if np.sum(rs) > 0:\n",
    "            no_completed += 1\n",
    "            success = True\n",
    "            \n",
    "        print('Success:', success)\n",
    "    \n",
    "    avg_reward /= float(no_runs)\n",
    "    \n",
    "    print()\n",
    "    print(f'average reward: {avg_reward}')\n",
    "    print(f'reached target: {no_completed} / {no_runs}')\n",
    "    \n",
    "    return no_completed, avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239132f0",
   "metadata": {},
   "source": [
    "### Implementing a policy\n",
    "\n",
    "For the simulator, $policy(observation) \\rightarrow action$ should be implemented as a plain python function which takes a numpy array as input (the observation) and returns an integer (the action).\n",
    "So generally, a policy implementation would look like this:\n",
    "\n",
    "```python\n",
    "\n",
    "def policy(observation):    \n",
    "    \"\"\"\n",
    "    Input: observation, a H x W x 3 numpy array containing an RGB image of the surroundings\n",
    "    Output: action,     an integer representing the action (0 = Left, ... 2 = Forward)\n",
    "    N.B.: actions is just an int, NOT a numpy array\n",
    "    \"\"\" \n",
    "    \n",
    "    # YOUR CODE\n",
    "    #   convert observation to feature vector\n",
    "    #   predict action class given the feature vector using some ML technique\n",
    "    \n",
    "    return action\n",
    "```\n",
    "*Of course, don't name you policy just `policy`, but give it some more descriptive name!*\n",
    "\n",
    "To illustrate, here is a dummy policy which just picks a random actions (without actually looking at the observation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "91446514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dummy policy\n",
    "def policy_dummy(observation):\n",
    "    \"\"\" Dummy policy function, which just returns random action. \n",
    "    \n",
    "    Input: observation, a H x W x 3 numpy array containing an RGB image of the surroundings\n",
    "    Output: action,     an integer representing the action (0 = Left, ... 2 = Forward)\n",
    "    N.B.: actions is just an int, NOT a numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    # in this dummy policy, we ignore the observation and just select a random action\n",
    "    action = np.random.randint(0, NUM_ACTIONS)\n",
    "    \n",
    "    # print(f'Received observation: {observation.shape} numpy array of type {observation.dtype}, returning action {action}')\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4667f5a1",
   "metadata": {},
   "source": [
    "We can confirm that the policy returns a valid action label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ad47b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = policy_dummy(observations)\n",
    "\n",
    "# returned action should be a builtin int or a numpy integer (NOT a numpy array) in the range [0, 2]\n",
    "assert (isinstance(action, (int, np.integer)))\n",
    "assert (action in ACTIONS) # ACTIONS is the set of possible action labels, [0,1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ab36b3",
   "metadata": {},
   "source": [
    "### Reward\n",
    "\n",
    "To quantify how well a policy is working, the simulator will return the *rewards* that the robot collected at each simulation step. The rewards determine how well you are doing, and are based on whether or not you reach the target (red cube) or not and how quickly you do so:\n",
    "\n",
    "* The robot receives a reward of 0 when not reaching the target within the given maximum number of steps.\n",
    "* The robot receives a reward of 1 when reaching the target, at which time also the episode ends. The robot also gets a small malus for the number of steps it needs to reach the target, as a penalty for spending time. The equation for this is: 1.0 - 0.2 * (step_count / max_episode_steps)\n",
    "\n",
    "Overall, the goal is to reach the red cube as quickly as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908609e",
   "metadata": {},
   "source": [
    "### Illustration of running the simulator with the dummy policy\n",
    "\n",
    "Let's try to run the simulation with the dummy policy, and render the output in the popup-window for 100 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "20d825fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation for 100 steps.\n",
      "*** Press ESC key in popup window to stop the simulation! ***\n",
      "\n",
      "total reward after 100 steps: 0\n",
      "average reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "time_id = 1 # <-- CHANGE THIS to select the time; can be 0, 1, 2\n",
    "\n",
    "if show_policy:\n",
    "    # running the simulation with the dummy policy\n",
    "    rs = run_simulation(policy_dummy, envs[time_id], max_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0918c2c8",
   "metadata": {},
   "source": [
    "Clearly, this policy does not do anything particularly useful, and should make the robot just move around randomly. Let's visualize the rewards that the robot collected during the simulation.\n",
    "\n",
    "The first plot below shows when the reward is collected. The second plot shows the total/cumulative reward. You will probably see two flat lines as the dummy policy did not accumulate any rewards. This is indicative of a bad policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760569cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if show_policy:\n",
    "    plt.figure(figsize=(10,4)) # create a wide figure (size 10) which is not so tall (size 4)\n",
    "    plt.subplot(1,2,1) # create subplot of 1 row, 2 columns, enable plotting in first cell\n",
    "    plt.plot(rs)\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('reward')\n",
    "    plt.title('Reward per step')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1,2,2) # create subplot of 1 row, 2 columns, enable plotting in first cell\n",
    "    plt.plot(np.cumsum(rs)) # Cumulative sum of rewards\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('total reward')\n",
    "    plt.title('Cumulative reward')\n",
    "    plt.grid()\n",
    "\n",
    "    print('Average reward:', np.mean(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e125f0",
   "metadata": {},
   "source": [
    "For comparing and evaluating classifiers, measure the performance of the classifiers themselves, i.e., the macro-F1 score.\n",
    "\n",
    "Still, the simulation and rewards can help you assess in what situations your robot AI is performing well, and when it is failing. For this use the function `test_policy`, with the default 10 runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0a381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_completed, avg_reward = test_policy(policy_dummy, envs_norender[time_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c9f70",
   "metadata": {},
   "source": [
    "# Collecting new demonstrations\n",
    "\n",
    "At some point, you might want to collect more human demonstration data to make your method even better.\n",
    "You can do this by:\n",
    "\n",
    "- manually controlling the robot in the simulation yourself to generate new demonstrations\n",
    "- recording the resulting (observation, action) pairs during these demonstrations\n",
    "- saving the good demonstrations to disk to increase your example dataset\n",
    "\n",
    "The code below demonstrates how to do this. The idea is simple: just use the regular `run_simulation()` function, but use a special `policy_human()` policy which simply returns the action based on the keyboard input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00df200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a human\n",
    "def policy_human(observation):\n",
    "    # Get the action obtained by the key_press/key_release callbacks from the popup window.\n",
    "    # Note that the human demonstrator (you!) will of course see the environment image in the popup window,\n",
    "    # and ignore the 'observation' input of this function.\n",
    "    # This 'human policy' will therefore return your 'action' response to the visual input by checking\n",
    "    # which keyboard arrows you pressed.\n",
    "    # Note 1: As we don't have an idle action, the time will only progress when you press a key\n",
    "    # Note 2: If you press and key except the arrow keys or ESC, the policy sanity check will fail\n",
    "\n",
    "    if KEY_PRESSED[key.LEFT]: return ACTIONS[0]\n",
    "    elif KEY_PRESSED[key.RIGHT]: return ACTIONS[1]\n",
    "    elif KEY_PRESSED[key.UP]: return ACTIONS[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f9349",
   "metadata": {},
   "source": [
    "When we use this policy, ensure that the simulator stores and returns all the (observation, action) pairs by setting the `record_data` argument of run_simulation to `True`.\n",
    "You can adjust the `time_id` to get human demonstrations at a variety of times during the day for the training environment. If you want a reproducible initial configuration, you can also set a `seed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7cf477",
   "metadata": {},
   "outputs": [],
   "source": [
    "if collect_demo:\n",
    "    time_id = 1 # <-- CHANGE THIS to select the time; can be 0, 1, 2\n",
    "    # NOTE: if an interactive widget is active, this simulation will run slower\n",
    "\n",
    "    rs, rec_obs, rec_actions = run_simulation(\n",
    "        policy_human,          # by using the 'human' policy, YOU determine the robot's actions based on what you see\n",
    "        record_data=True,      # record and return all (observation, action) pairs from the simulation \n",
    "        env=envs[time_id],     # which environment to use\n",
    "        max_steps=1000,        # maximum number of steps to run the simulation\n",
    "        human_control=True,    # when controlling the robot manually\n",
    "        delay=0.07,            # adding a small delay will help you control the robot\n",
    "    )\n",
    "    # Note: with delay=0.05 the simulation runs a bit slower, which makes it easier to give demonstrations\n",
    "    \n",
    "else:\n",
    "    rec_obs = observations\n",
    "    rec_actions = actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DEMO = True # <-- CHANGE THIS to 'True' to SAVE the last recording to disk!\n",
    "\n",
    "if SAVE_DEMO: \n",
    "    rec_N = rec_obs.shape[0]\n",
    "\n",
    "    demonstration = {\n",
    "        'observations': rec_obs,\n",
    "        'actions': rec_actions,\n",
    "        'time': time_id * np.ones(rec_N, dtype=int),\n",
    "    }\n",
    "    \n",
    "    # include date+time to filename in YYYYMMDD_HHMMSS format\n",
    "    import datetime\n",
    "    now = datetime.datetime.now()\n",
    "    dt_str = now.strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    # Save to disk\n",
    "    save_filename = f'demonstrations/demostud-{time_id}-{dt_str}.pickle'\n",
    "    print(f'Saving demonstration of time {time_id} to {save_filename} ...')\n",
    "    with open(save_filename, 'wb') as fd:\n",
    "        pickle.dump(demonstration, fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b759d7a7",
   "metadata": {},
   "source": [
    "Explore the just collected samples using an interactive slider (if you didn't collect new data you'll explore the pre-recoded demonstrations instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(idx, observations, actions):\n",
    "    obs = observations[idx]\n",
    "    action = actions[idx]\n",
    "    plt.clf()\n",
    "    plt.imshow(obs)\n",
    "    plt.title(f'{idx}: {action}');\n",
    "\n",
    "ipywidgets.interactive(lambda idx: plot_sample(idx, rec_obs, rec_actions),\n",
    "                       idx=(0,rec_obs.shape[0]-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8cf8de",
   "metadata": {},
   "source": [
    "If you are unhappy with the the demonstration you gave, you can just execute the `run_simulation()` cell above again, until you are satisfied.\n",
    "\n",
    "To save the demonstration to disk, execute the cell below after setting `SAVE_DEMO` to True.\n",
    "\n",
    "**After you have saved the demonstration, don't forget to afterwards IMMEDIATELY set `SAVE_DEMO` back to False to avoid accidentally saving new demonstrations every time you rerun the notebook!!!**\n",
    "\n",
    "Note that the pickle filenames of your recordings will start with `demostud-`, while the originally provided demonstrations start with `demo-`. This makes it easy to load only the original, your, or both types of recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e67b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DEMO = True # <-- CHANGE THIS to 'True' to SAVE the last recording to disk!\n",
    "\n",
    "if SAVE_DEMO: \n",
    "    rec_N = rec_obs.shape[0]\n",
    "\n",
    "    demonstration = {\n",
    "        'observations': rec_obs,\n",
    "        'actions': rec_actions,\n",
    "        'time': time_id * np.ones(rec_N, dtype=int),\n",
    "    }\n",
    "    \n",
    "    # include date+time to filename in YYYYMMDD_HHMMSS format\n",
    "    import datetime\n",
    "    now = datetime.datetime.now()\n",
    "    dt_str = now.strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    # Save to disk\n",
    "    save_filename = f'demonstrations/demostud-{time_id}-{dt_str}.pickle'\n",
    "    print(f'Saving demonstration of time {time_id} to {save_filename} ...')\n",
    "    with open(save_filename, 'wb') as fd:\n",
    "        pickle.dump(demonstration, fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67f75fe",
   "metadata": {},
   "source": [
    "Ok, that completes the example code.\n",
    "Now it is your turn! Implement your solution to the final assignment below. For full points, make sure you address *all* the numbered items for each section, either by implementing something in code cells, or by providing text in Markdown cells. You are free to add as many code and markdown cells as required. Be sure to first read through all sections before you start, so you know what should go where. We are *not* using nbgrader for this final assignment.\n",
    "\n",
    "**When you are done, double check the \"Deliverables\" section at the start of this notebook on how to prepare your final submission!**\n",
    "\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1edc05e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Explore & Inspect the Data (5 points)\n",
    "In this section you should only use the ORIGINALLY provided demo files (i.e. 'demo-*-*.pickle')\n",
    "\n",
    "Add code and markdown cells to address all of the following points:\n",
    "\n",
    "1. Create a visualization that shows three samples from each time of day for which you have demonstrations. <span style=\"float: right;\">(0.5 pt)</span>\n",
    "2. Explain in words what you observe: how do the observations from the times of day vary? <span style=\"float: right;\">(0.5 pt)</span>\n",
    "3. Are the samples i.i.d.? What does that imply for splitting your data? <span style=\"float: right;\">(1 pt)</span>\n",
    "4. Is there a class imbalance? If yes, what are procedures to deal with that? <span style=\"float: right;\">(1 pt)</span>\n",
    "5. Do we have a high risk of conflicting labels for observations? What problems can this cause? <span style=\"float: right;\">(1 pt)</span>\n",
    "6. The data was collected from human demonstrations. What are potential issues with this way of collecting data? <span style=\"float: right;\">(1 pt)</span>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd54d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for all the demonstration pickle files in the demonstrations/ directory.\n",
    "#  The originally provided demo files are called: demo-[time_id]-[datetime].pickle\n",
    "#  Any demo files you record yourself are called: demostud-[time_id]-[datetime].pickle\n",
    "\n",
    "# CHANGE THIS IF NEEDED: select the pickle file pattern to match ...\n",
    "\n",
    "DEMO_FILEPATTERN = 'demo-*-*.pickle'      # only use ORIGINALLY provided demo files\n",
    "#DEMO_FILEPATTERN = 'demostud-*-*.pickle'  # only use YOUR own collected demo files\n",
    "#DEMO_FILEPATTERN = 'demo*-*.pickle'        # use ALL available demo files\n",
    "\n",
    "# find the relevant filenames\n",
    "filenames = glob.glob(f'demonstrations/{DEMO_FILEPATTERN}')\n",
    "filenames.sort() # ensure the order is well-defined\n",
    "print(f'Found {len(filenames)} demonstrations')\n",
    "\n",
    "# in a loop, load the found pickle files\n",
    "demonstrations = []\n",
    "for filename in filenames:\n",
    "    with open(filename, 'rb') as fd:\n",
    "        demonstration = pickle.load(fd)\n",
    "        \n",
    "        actions = demonstration['actions']\n",
    "        demonstrations.append(demonstration)\n",
    "        \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ba663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can combine all the observations, actions, and time ids\n",
    "\n",
    "observations = np.concatenate([d['observations'] for d in demonstrations])\n",
    "actions = np.concatenate([d['actions'] for d in demonstrations])\n",
    "time_ids = np.concatenate([d['time'] for d in demonstrations])\n",
    "\n",
    "# pre-processing: subsample and only keep every n-th sample for efficiency\n",
    "# this can speed up training\n",
    "ss = 1\n",
    "observations = observations[::ss]\n",
    "actions = actions[::ss]\n",
    "time_ids = time_ids[::ss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef75732",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = []\n",
    "idx.append(list(range(4,7)))\n",
    "idx.append(list(range(int(time_ids.shape[0]/2), int(time_ids.shape[0]/2)+3)))\n",
    "idx.append(list(range(time_ids.shape[0]-3, time_ids.shape[0])))\n",
    "\n",
    "fig, axs = plt.subplots(3, 3)\n",
    "\n",
    "\n",
    "\n",
    "for j in range(3):\n",
    "    for i in range(3):\n",
    "        axs[j, i].imshow(observations[idx[j][i]])\n",
    "        axs[j, i].set_title(f\"Time_id: {time_ids[idx[j][i]]}, img_id: {idx[j][i]}\")\n",
    "        axs[j, i].title.set_size(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52089e61",
   "metadata": {},
   "source": [
    "# 1.2 Observations from data\n",
    "\n",
    "When examining the images gathered from the observations. the following similarities and differences can be found:\n",
    "\n",
    "1. For each time of day the environment regarding cones and walls do not change\n",
    "2. For each new sample the position of the red cube may change\n",
    "3. For each sample the intial orientation of the robot changes\n",
    "4. The images for each time of day only differ in **hue**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ba43e",
   "metadata": {},
   "source": [
    "# 1.3 Are the samples i.i.d?\n",
    "\n",
    "The demo files in the demonstration folder are sorted from time id 0 to 2, and the list 'observations' simply concatenates all the observations within demonstrations. This means that our observation samples will also be sorted from timeid 0 to 2. Resulting in a sample that is **not** i.i.d.\n",
    "\n",
    "Splitting the data mean taking every n-th element of a list. In our case since the list is sorted, if we split it the resulting list will have the same distribution of time ids as the original list. However this is still not i.i.d. therefore we should shuffle the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b51f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_time0 = np.count_nonzero(time_ids == 0)\n",
    "N_time1 = np.count_nonzero(time_ids == 1)\n",
    "N_time2 = np.count_nonzero(time_ids == 2)\n",
    "\n",
    "print(f\"The number of samples with timeid 0:  {N_time0}\")\n",
    "print(f\"The number of samples with timeid 1:  {N_time1}\")\n",
    "print(f\"The number of samples with timeid 2:  {N_time2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b5ee42",
   "metadata": {},
   "source": [
    "# 1.4 Is there a class imbalance?\n",
    "\n",
    "There is a class imbalance in our observations as there are fewer samples of timeid = 2 compared to timeid = 0 and 1. This can be by adressed by gathering more data with timeid = 2. \n",
    "\n",
    "If gathering more samples is not possible, a different option is to undersample the majority class. In our case this means taking fewer samples of timeid 0 and 1 to match that of timeid 2. This however means a loss of information.\n",
    "\n",
    "Similarly you can also oversample timeid 2, so duplicating random samples with timeid 2 to increase the sample size. This however can result in overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b78366b",
   "metadata": {},
   "source": [
    "## 1.5 Do we have a high risk of conflicting labels for observations? What problems can this cause?\n",
    "\n",
    "As all data is generated by humans, a lot of different actions are taken in (basically) the same observation. This could cause conflicting labels, as the training data itself is conflicting. The noise by measurement techniques or in sensor data will be minimal though, which means that all noise probably comes from theÂ humanÂ operator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6443b",
   "metadata": {},
   "source": [
    "## 1.6 The data was collected from human demonstrations. What are potential issues with this way of collecting data?\n",
    "\n",
    "By using humans to generate data it most certainly will not be the most efficient path to the red cube, as humans often do not take the shortest path to the red cube. Using human generated data could also mean a lot of conflicting data, as mentioned before, where some human might take a left, others mightÂ takeÂ aÂ right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1a0f4",
   "metadata": {},
   "source": [
    "# ---\n",
    "\n",
    "# 2. Prepare the Data and Evaluate Features (15 points)\n",
    "\n",
    "In this section you should only use the ORIGINALLY provided demo files (i.e. 'demo-*-*.pickle'). You should pre-process the data, e.g., down-sample, and extract features to create your training data matrix \"X\".\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Remember to provide thorough insights, analysis and reasoning behind your design choices and what you see as a consequence of these choices. Try to synthesize rather than describing what you did step by step.\n",
    "</div>\n",
    "\n",
    "## 2.1. Clustering observations from times of day (6 points)\n",
    "\n",
    "Before we turn towards the main task of action classification (section 2.2), let us first try a small unsupervised clustering task. Pretend that we only have the observations, but did not record the time_ids of these observations. The goal is to cluster the observations into k=3 clusters such that 1 cluster (approximately) corresponds to 1 time of day. For this task, you can ignore the actions and time_id information.\n",
    "\n",
    "1. Propose a feature extraction method `feat_extract_clust` which can be used to CLUSTER the samples and (approximately)<br> recover the time_ids. Motivate what you use in your feature extraction method. <span style=\"float: right;\">(2 pt)</span>\n",
    "2. Perform clustering based on the features obtained with `feat_extract_clust`, and compare the results to the true time_id<br> labels. For this you will need to select a statistical measure to compare cluster labels to time_ids. <span style=\"float: right;\">(1 pt)</span>\n",
    "3. Explain what measure you use for comparing the features and why. <span style=\"float: right;\">(2 pt)</span>\n",
    "4. Can you recover the time_ids by clustering? Motivate your answer with your results. <span style=\"float: right;\">(1 pt)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ad431",
   "metadata": {},
   "source": [
    "# 2.1.1 Feature extraction method\n",
    "\n",
    "Considering we can't use the timeids to determine the time of day, we have to do feature extraction on the RGB images.\n",
    "\n",
    "Because the the images only differ in hue, we can make the image feature transformation from RGB format to HSV format. Where HSV is hue, saturation and value. The transformation from RGB to HSV does not affect the dimensionality of the feature space, therefore making it a suitable option for clustering based on hues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11bfe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.2 Clustering\n",
    "\n",
    "from matplotlib.colors import rgb_to_hsv\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "shuffled_obs, shuffled_actions, shuffled_time_ids = shuffle(observations, actions, time_ids, random_state=0)\n",
    "\n",
    "def feat_extract_clust(observations):\n",
    "    obs_f = []\n",
    "    for obs in observations:\n",
    "        # Convert RGB to HSV, extract only H value and flatten image to feature vector\n",
    "        obs_f.append(rgb_to_hsv(obs)[:,:,0].flatten())\n",
    "    \n",
    "    # Run KMeans on the modified set\n",
    "    kmeans = KMeans(n_clusters=3, random_state=2)\n",
    "    y_pred_km = kmeans.fit_predict(obs_f)\n",
    "    return y_pred_km\n",
    "\n",
    "y_pred_km = feat_extract_clust(shuffled_obs)\n",
    "\n",
    "print(classification_report(shuffled_time_ids, y_pred_km))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e33c7",
   "metadata": {},
   "source": [
    "# 2.1.3 Method for comparing features\n",
    "\n",
    "We made the feature transformation from RGB to HSV and did clustering based on the hue of the image. When comparing features, we can compare the hue of two samples. \n",
    "\n",
    "Where colours in RGB  is defined in a 'colour cube' (meaning R, G and B are all linear), HSV is defined in a colour hexdome. Here the hue is defined in a circular colour space, and the saturation and value are linear. Meaning that when we want to compare two different hues, we should take the angular distance between them. A small angular distance between two features mean they have similar hue and a large angular distance means that the hue is different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2a235",
   "metadata": {},
   "source": [
    "# 2.1.4 Can you recover time_ids by clustering?\n",
    "\n",
    "Observing the classification report, the accuracy of the KMeans clustering is fairly high at 0.92 but is not perfect. With more fine-tuning this accuracy can be improved however likely not to an accuracy of 1.0. Therefore the time_ids can not be recovered by clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144d5957",
   "metadata": {},
   "source": [
    "## 2.2. Features for action classification (9 points)\n",
    "\n",
    "Now we turn to feature extraction for the main classification task, which you can reuse also in the later sections of this notebook.\n",
    "\n",
    "1. Explain: Will you use the same extractor as in step 1; Why (not)? <span style=\"float: right;\">(2 pt)</span>\n",
    "2. Propose a feature extraction method `feat_extract` which you will use in the subsequent sections to classify *action*, rather<br> than time_ids. <span style=\"float: right;\">(1 pt)</span>\n",
    "3. Explain: Are there any important hyperparameters in your feature extractor? <span style=\"float: right;\">(2 pt)</span>\n",
    "4. Explain: How will you decide on the values for these hyperparameters? What is the trade-off if this hyperparameter is either (too)<br> low or (too) high? <span style=\"float: right;\">(2.5 pt)</span>\n",
    "5. Explain: What is the dimensionality of your feature space? <span style=\"float: right;\">(0.5 pt)</span>\n",
    "6. Apply your `feat_extract` to all observations to create the data that you will use in the subsequent sections. <span style=\"float: right;\">(1 pt)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e08ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code and markdown cells\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09b60f",
   "metadata": {},
   "source": [
    "### 2.2.1 Explain: Will you use the same extractor as in step 1; Why (not)?\n",
    "\n",
    "No, we will not use the same feature transformation. Because the previous extractor was purely to determine the time_id. To determine the correct action the robot must take different features are of importance. Therefore a different feature extraction method must be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c077643",
   "metadata": {},
   "source": [
    "### 2.2.2 Propose a feature extraction method feat_extract which you will use in the subsequent sections to classify action, rather than time_ids.\n",
    "\n",
    "Because the cube is red and quite distinct from its environment regarding colour, a viable feature extraction method would be colour image segmentation in the HSV colour space. From there you could recognise the cube and create a mask from it. By training a classifier with these masks, the classifier will learn to walk towards the red cube.\n",
    "\n",
    "Additionally we could also provide the classifier with a grayscale image. This to prevent walking into the wall or off the sidewalk. A grayscale image would be better suited than the original image, as removing color would reduce it's dimension (which boosts training performance) and will also weaken the impact of the different time of days, as the impact of color difference will be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d771ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 100\n",
    "\n",
    "obs_hsv = rgb_to_hsv(observations[idx])\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15,5))\n",
    "ax[0].imshow(obs_hsv[:,:,0], cmap='gray')\n",
    "ax[0].set_title('H-value')\n",
    "ax[1].imshow(obs_hsv[:,:,1], cmap='gray')\n",
    "ax[1].set_title('S-value')\n",
    "ax[2].imshow(obs_hsv[:,:,2], cmap='gray')\n",
    "ax[2].set_title('V-value')\n",
    "ax[3].imshow(np.dot(observations[idx][...,:3], [0.299, 0.587, 0.114]), cmap='gray')\n",
    "ax[3].set_title('Grayscale')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af47d0f3",
   "metadata": {},
   "source": [
    "### 2.2.3 Explain: Are there any important hyperparameters in your feature extractor?\n",
    "\n",
    "For colour image segmentation in HSV color space we first want to convert the image into HSV color space like was done previously. To implement the segmenting we need to define some thresholds for hue and saturation. These thresholds determine which parts of the image we want to include in the mask and are thus hyperparameters.\n",
    "\n",
    "By experimenting with the threshold values in the code cell below we can determine these hyperparameters.\n",
    "\n",
    "For the grayscale conversion the hyperparameters might be less obvious, but we can also tweak the formula for converting RGB to grayscale. As the standard grayscale conversion parameters work sufficiently for now, we'll continue with those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb93dc",
   "metadata": {},
   "source": [
    "### 2.2.4 Explain: How will you decide on the values for these hyperparameters? What is the trade-off if this hyperparameter is either (too) low or (too) high?\n",
    "\n",
    "As our red cube is represented by a low hue and a high saturation, as can be seen in the images above, we will want to start with a mask that separates everything under a low H-threshold and everything above a high S-threshold and work our way until a red cube becomes visible.\n",
    "\n",
    "Depending on how broad you define the threshold you will accept more or less colours in the feature extraction. A larger threshold will also allow orange hues for example. A smaller threshold on the other hand will only accept 'pure' red colours. \n",
    "\n",
    "This can become an issue with the cones, as the cones are orange, which comes close to our red. It can be seen in the images above both the cones and the cube have similar values for H and S."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c1960",
   "metadata": {},
   "source": [
    "### 2.2.5 Explain: what is the dimensionality of your feature space\n",
    "\n",
    "As we will generate a mask and a grayscale image as our features (both images with one value per pixel), where both images can be flattened to feature vectors (1-dim), our feature space will be 2-dim as 2x 1-dim vectors make a 2-dim feature space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12582a0b",
   "metadata": {},
   "source": [
    "### 2.2.6 Apply your feat_extract to all observations to create the data that you will use in the subsequent sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8cc780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color segmentation\n",
    "#\n",
    "# As you can see, using the HSV color space produces some promising results for segmenting the red cube.\n",
    "# Only once time_id 2 is reached this falls apart, as the gray color of the sidewalk gets detected.\n",
    "# As we only focus on time_id 1 for now, we keep this issue to solve another time\n",
    "\n",
    "threshold_h = 0.02\n",
    "threshold_s = 0.34\n",
    "\n",
    "def calculate_com(mask):\n",
    "    if not np.any(mask):\n",
    "        # return none if mask is empty\n",
    "        return None    \n",
    "    \n",
    "    mass_x, mass_y = np.where(mask)\n",
    "    # Use median instead of average to be more noise robust\n",
    "    cent_x, cent_y = np.median(mass_x), np.median(mass_y)\n",
    "    return int(cent_x), int(cent_y)\n",
    "\n",
    "def cube_mask(obs, threshold_h, threshold_s):\n",
    "    # Convert observation to HSV and generate a mask where H < threshold_h\n",
    "    obs_hsv = rgb_to_hsv(obs)\n",
    "    h_mask = obs_hsv[:,:,0] < threshold_h\n",
    "    s_mask = obs_hsv[:,:,1] > threshold_s\n",
    "    mask = h_mask*s_mask\n",
    "    return mask\n",
    "    \n",
    "    \n",
    "def plot_sample(idx, observations, threshold_h):\n",
    "    obs = observations[idx]\n",
    "    mask = cube_mask(obs, threshold_h, threshold_s)\n",
    "\n",
    "    # Generate figures showing original, H-value and mask\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(15,5))\n",
    "    ax[0].imshow(obs)\n",
    "    ax[0].set_title(f'Observation in HSV: {idx}')\n",
    "    ax[1].imshow(rgb_to_hsv(obs)[:,:,0], cmap='gray')\n",
    "    ax[1].set_title('H-value')\n",
    "    ax[2].imshow(rgb_to_hsv(obs)[:,:,1], cmap='gray')\n",
    "    ax[2].set_title('S-value')\n",
    "    ax[3].imshow(mask)\n",
    "    ax[3].set_title(f'Mask with threshold {threshold_h}')\n",
    "    \n",
    "    \n",
    "    # To use basic logic to steer we can calculate the Center of Mass, but not needed when training a classifier.\n",
    "    # We now know how to detect the red cube. \n",
    "    # To steer towards it we need to calculate the center of the mask\n",
    "    # If the red cube is not in frame, do not calculate a CoM.\n",
    "    if np.any(mask):\n",
    "        com = calculate_com(mask)\n",
    "        ax[3].plot(com[1], com[0], 'r+', markersize=15)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "ipywidgets.interactive(lambda idx: plot_sample(idx, observations, threshold_h),\n",
    "                       idx=(0,observations.shape[0]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f49b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_h = 0.02\n",
    "idx=10\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def feat_extract(obs):\n",
    "    # Extract all necessary features for classifier training\n",
    "    # Currently we use \n",
    "    #   1. Mask generated from HSV \n",
    "    #   2. Grayscale image\n",
    "    features = []\n",
    "    mask = cube_mask(obs, threshold_h, threshold_s)\n",
    "    scaler = StandardScaler()\n",
    "    mask_scaled = scaler.fit_transform(mask)\n",
    "    features.append(mask.flatten())\n",
    "    \n",
    "    grayscale = np.dot(obs[...,:3], [0.299, 0.587, 0.114]) \n",
    "    scaler = StandardScaler()\n",
    "    grayscale_scaled = scaler.fit_transform(grayscale)\n",
    "    \n",
    "    features.append(grayscale_scaled.flatten())\n",
    "    return np.concatenate(features)\n",
    "\n",
    "features = feat_extract(observations[idx])\n",
    "# print(features)\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "# ax[0].imshow(np.reshape(features[0],(60,80)), cmap='gray')\n",
    "# ax[0].set_title('Mask')\n",
    "# ax[1].imshow(np.reshape(features[1],(60,80)), cmap='gray')\n",
    "# ax[1].set_title('Grayscale image')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a8d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all features from each observation and put them together in extracted_obs\n",
    "\n",
    "def feat_extract_all(observations):\n",
    "    # extracted_obs index:\n",
    "    # extracted_obs = list[obs_np_array_i[hsv_mask_np_array[], gray_np_array[]]]\n",
    "    extracted_obs = [feat_extract(obs) for obs in observations]\n",
    "    return np.array(extracted_obs)\n",
    "\n",
    "print(feat_extract_all(observations).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d131ef",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# 3. Single Time of Day Action Classification  (35 points)\n",
    "To get started, we will train and test a model that is only suitable for operating at a single time of day, i.e., at Time 1\n",
    "\n",
    "As a first step split the data into a training, validation, and test set. You can use the originally provided data, or collect your own.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Remember to provide thorough insights, analysis and reasoning behind your design choices and what you see as a consequence of these choices. Try to synthesize rather than describing what you did step by step.\n",
    "</div>\n",
    "\n",
    "\n",
    "## 3.1. Shortlist Promising Models (25 points)\n",
    "1. Compare at least 2 models. One of them needs to be a neural network, one of them needs to be not a neural network.<span style=\"float: right;\">(3 pt)</span>\n",
    "2. For each of the models that you are going to compare, explain what are its relative advantages/disadvantages in terms of training<br> time, test time, and number of model parameters compared to the other choices. Also explain how these considerations relate to<br> the target application, and motivate which type of model would be preferred based on these considerations only (so disregarding<br> the actual quality of the models). <span style=\"float: right;\">(10 pt)</span>\n",
    "3. If needed, perform dimensionality reduction before training your selection models. Explain why it is (not) needed.<span style=\"float: right;\">(1.5 pt)</span>\n",
    "4. Roughly tune those models<span style=\"float: right;\">(1 pt)</span>\n",
    "5. Evaluate the models in terms of performance, bias, variance, etc.<span style=\"float: right;\">(5 pt)</span>\n",
    "6. Please use the macro-F1 score (see sklearn documentation) as your main criterion. Looking at confusion matrices, accuracy etc.<br> might also provide valuable insights. Why is the accuracy score potentially problematic in this setting?<span style=\"float: right;\">(1.5 pt)</span>\n",
    "7. Pick one algorithm to develop further. Motivate your choice of algorithm with your results.<span style=\"float: right;\">(3 pt)</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d15bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for all the demonstration pickle files in the demonstrations/ directory.\n",
    "#  The originally provided demo files are called: demo-[time_id]-[datetime].pickle\n",
    "#  Any demo files you record yourself are called: demostud-[time_id]-[datetime].pickle\n",
    "\n",
    "# CHANGE THIS IF NEEDED: select the pickle file pattern to match ...\n",
    "\n",
    "#DEMO_FILEPATTERN = 'demo-*-*.pickle'      # only use ORIGINALLY provided demo files\n",
    "#DEMO_FILEPATTERN = 'demostud-*-*.pickle'  # only use YOUR own collected demo files\n",
    "DEMO_FILEPATTERN = 'demo*-*.pickle'        # use ALL available demo files\n",
    "\n",
    "# find the relevant filenames\n",
    "filenames = glob.glob(f'demonstrations/{DEMO_FILEPATTERN}')\n",
    "filenames.sort() # ensure the order is well-defined\n",
    "print(f'Loading {len(filenames)} demonstrations')\n",
    "\n",
    "# in a loop, load the found pickle files\n",
    "demonstrations = []\n",
    "for filename in filenames:\n",
    "    with open(filename, 'rb') as fd:\n",
    "        demonstration = pickle.load(fd)\n",
    "        \n",
    "        actions = demonstration['actions']\n",
    "        #print(f'Loaded {actions.shape[0]} samples from {filename} ...')\n",
    "        \n",
    "        demonstrations.append(demonstration)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80842851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can combine all the observations, actions, and time ids\n",
    "\n",
    "observations = np.concatenate([d['observations'] for d in demonstrations if any(d['time'] == 1)])\n",
    "actions = np.concatenate([d['actions'] for d in demonstrations if any(d['time'] == 1)])\n",
    "time_ids = np.concatenate([d['time'] for d in demonstrations if any(d['time'] == 1)])\n",
    "\n",
    "observations = np.concatenate([d['observations'] for d in demonstrations])\n",
    "actions = np.concatenate([d['actions'] for d in demonstrations])\n",
    "time_ids = np.concatenate([d['time'] for d in demonstrations])\n",
    "\n",
    "# pre-processing: subsample and only keep every n-th sample for efficiency\n",
    "# this can speed up training\n",
    "ss = 1\n",
    "observations = observations[::ss]\n",
    "actions = actions[::ss]\n",
    "time_ids = time_ids[::ss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22f4141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3903666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = feat_extract_all(observations)\n",
    "#features_stacked = np.array([(np.ones(len(feature[0]))-feature[0])*feature[1] for feature in features])\n",
    "\n",
    "features_stacked = np.array([np.hstack((feature[0], feature[1])) for feature in features])\n",
    "print(np.max(features))\n",
    "\n",
    "print(features_stacked.shape)\n",
    "print(actions.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, actions, test_size = 0.20, random_state = 0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=9600, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63f86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer,loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_onehot = to_categorical(y_train, num_classes=3)\n",
    "y_test_onehot = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "model.fit(X_train, y_train_onehot, epochs=30)\n",
    "\n",
    "model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "# loss, accuracy = model.evaluate(X_test, y_test_onehot)\n",
    "# print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "preds = []\n",
    "\n",
    "for pred in predictions:\n",
    "    preds.append(np.argmax(pred))\n",
    "\n",
    "print(classification_report(preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c7a5e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_action = 0\n",
    "\n",
    "def policy_nn(observation):\n",
    "    global prev_action\n",
    "    \"\"\"\n",
    "    Calculate the location of the center of mass of the cube. \n",
    "    - If it's X-location is inside the middle of the observation; walk forward. \n",
    "    - If the CoM is to the left of the screen: turn right\n",
    "    - If the CoM is to the right of the screen: turn left\n",
    "    - Else: turn right\n",
    "    \n",
    "    Considering cube_com has shape (2, )\n",
    "    \"\"\"\n",
    "    feat = feat_extract(observation)\n",
    "\n",
    "    action = model.predict(np.reshape(feat,(1,9600)))\n",
    "    action = np.argmax(action[0])\n",
    "#     if action == 0 and prev_action == 1:\n",
    "#         prev_action = action\n",
    "#         return 2\n",
    "#     if action == 1 and prev_action == 0:\n",
    "#         prev_action = action\n",
    "#         return 2\n",
    "#     prev_action = action\n",
    "    print(action)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "25c98507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation for 100 steps.\n",
      "*** Press ESC key in popup window to stop the simulation! ***\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "2\n",
      "total reward after 93 steps: 0.814\n",
      "average reward: 0.00875268817204301\n"
     ]
    }
   ],
   "source": [
    "rs = run_simulation(policy_nn, envs[time_id], max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37010eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d328a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def trainRF(rf_clf, observations, mask:bool = True,  grayscale:bool = True, test_size = 0.25, random_state = 0):\n",
    "    # Function to train the RandomForestClassifier, the booleans 'mask' and 'grayscale'\n",
    "    # determine whether the dataset X will also include those features\n",
    "    \n",
    "    rnf_clf = rf_clf\n",
    "    all_features = feat_extract_all(observations)\n",
    "    y = shuffled_actions\n",
    "    print(all_features.shape)\n",
    "    if mask and grayscale:\n",
    "        X = all_features\n",
    "    elif not grayscale:\n",
    "        X = all_features[:, :4800]\n",
    "    elif not mask:\n",
    "        X = all_features[:, 4800:]\n",
    " \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = random_state)\n",
    "    rnf_clf.fit(X_train, y_train)\n",
    "    y_pred_rf = rnf_clf.predict(X_test)\n",
    "    \n",
    "    print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e8d009ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for only mask\n",
      "(522, 9600)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        33\n",
      "           1       0.33      0.06      0.10        18\n",
      "           2       0.61      0.96      0.75        80\n",
      "\n",
      "    accuracy                           0.60       131\n",
      "   macro avg       0.31      0.34      0.28       131\n",
      "weighted avg       0.42      0.60      0.47       131\n",
      "\n",
      "Classification report for only grayscale\n",
      "(522, 9600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dielof\\anaconda3\\envs\\homl3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Dielof\\anaconda3\\envs\\homl3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Dielof\\anaconda3\\envs\\homl3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        33\n",
      "           1       0.00      0.00      0.00        18\n",
      "           2       0.60      0.97      0.75        80\n",
      "\n",
      "    accuracy                           0.60       131\n",
      "   macro avg       0.20      0.33      0.25       131\n",
      "weighted avg       0.37      0.60      0.46       131\n",
      "\n",
      "Classification report for mask and grayscale\n",
      "(522, 9600)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        33\n",
      "           1       0.00      0.00      0.00        18\n",
      "           2       0.61      0.99      0.75        80\n",
      "\n",
      "    accuracy                           0.60       131\n",
      "   macro avg       0.20      0.33      0.25       131\n",
      "weighted avg       0.37      0.60      0.46       131\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dielof\\anaconda3\\envs\\homl3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Dielof\\anaconda3\\envs\\homl3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Dielof\\anaconda3\\envs\\homl3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Define RandomForestClassifiers for the mask dataset, grayscale dataset and both\n",
    "rf_clf_mask = RandomForestClassifier(n_estimators = 500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rf_clf_gray = RandomForestClassifier(n_estimators = 500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rf_clf_both = RandomForestClassifier(n_estimators = 500, max_leaf_nodes=16, n_jobs=-1)\n",
    "\n",
    "# Train each classifier with its corresponding dataset\n",
    "print(\"Classification report for only mask\")\n",
    "trainRF(rf_clf_mask, observations, True, False)\n",
    "print(\"Classification report for only grayscale\")\n",
    "trainRF(rf_clf_gray, observations, False, True)\n",
    "print(\"Classification report for mask and grayscale\")\n",
    "trainRF(rf_clf_both, observations, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "83cf3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steering mechanism\n",
    "def policy_mask(observation):    \n",
    "    \"\"\"\n",
    "    Calculate the location of the center of mass of the cube. \n",
    "    - If it's X-location is inside the middle of the observation; walk forward. \n",
    "    - If the CoM is to the left of the screen: turn right\n",
    "    - If the CoM is to the right of the screen: turn left\n",
    "    - Else: turn right\n",
    "    \n",
    "    Considering cube_com has shape (2, )\n",
    "    \"\"\"\n",
    "    mask = np.transpose(cube_mask(observation, threshold_h, threshold_s).flatten())\n",
    "    mask = mask.reshape(1, -1)\n",
    "    feat = feat_extract(observation)[:4800]\n",
    "    \n",
    "    action_list = rf_clf_mask.predict(np.reshape(feat,(1,4800)))\n",
    "    action = action_list[0]\n",
    "    \n",
    "    # YOUR CODE\n",
    "    #   convert observation to feature vector\n",
    "    #   predict action class given the feature vector using some ML technique\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "80a40b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steering mechanism\n",
    "def policy_gray(observation):    \n",
    "    \"\"\"\n",
    "    Calculate the location of the center of mass of the cube. \n",
    "    - If it's X-location is inside the middle of the observation; walk forward. \n",
    "    - If the CoM is to the left of the screen: turn right\n",
    "    - If the CoM is to the right of the screen: turn left\n",
    "    - Else: turn right\n",
    "    \n",
    "    Considering cube_com has shape (2, )\n",
    "    \"\"\"\n",
    "    mask = np.transpose(cube_mask(observation, threshold_h, threshold_s).flatten())\n",
    "    mask = mask.reshape(1, -1)\n",
    "    \n",
    "    action_list = rf_clf_gray.predict(mask)\n",
    "    action = action_list[0]\n",
    "    \n",
    "    # YOUR CODE\n",
    "    #   convert observation to feature vector\n",
    "    #   predict action class given the feature vector using some ML technique\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "e26f96e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steering mechanism\n",
    "def policy_both(observation):    \n",
    "    \"\"\"\n",
    "    Calculate the location of the center of mass of the cube. \n",
    "    - If it's X-location is inside the middle of the observation; walk forward. \n",
    "    - If the CoM is to the left of the screen: turn right\n",
    "    - If the CoM is to the right of the screen: turn left\n",
    "    - Else: turn right\n",
    "    \n",
    "    Considering cube_com has shape (2, )\n",
    "    \"\"\"\n",
    "    mask = np.transpose(cube_mask(observation, threshold_h, threshold_s).flatten())\n",
    "    mask = mask.reshape(1, -1)\n",
    "    feat = feat_extract(observation)\n",
    "    \n",
    "    action_list = rf_clf_both.predict(np.reshape(feat,(1,9600)))\n",
    "    action = action_list[0]\n",
    "    \n",
    "    # YOUR CODE\n",
    "    #   convert observation to feature vector\n",
    "    #   predict action class given the feature vector using some ML technique\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "3704316b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation for 100 steps.\n",
      "*** Press ESC key in popup window to stop the simulation! ***\n",
      "\n",
      "total reward after 16 steps: 0\n",
      "average reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "time_id = 1 # <-- CHANGE THIS to select the time; can be 0, 1, 2\n",
    "\n",
    "if show_policy:\n",
    "    # running the simulation with the requested policy (policy_mask, policy_gray or policy_both)\n",
    "    rs = run_simulation(policy_mask, envs[time_id], max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdc5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_policy:\n",
    "    plt.figure(figsize=(10,4)) # create a wide figure (size 10) which is not so tall (size 4)\n",
    "    plt.subplot(1,2,1) # create subplot of 1 row, 2 columns, enable plotting in first cell\n",
    "    plt.plot(rs)\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('reward')\n",
    "    plt.title('Reward per step')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1,2,2) # create subplot of 1 row, 2 columns, enable plotting in first cell\n",
    "    plt.plot(np.cumsum(rs)) # Cumulative sum of rewards\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('total reward')\n",
    "    plt.title('Cumulative reward')\n",
    "    plt.grid()\n",
    "\n",
    "    print('Average reward:', np.mean(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_completed, avg_reward = test_policy(policy_mask, envs_norender[time_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f3acad",
   "metadata": {},
   "source": [
    "## 3.2. Fine-Tune the System (10 points)\n",
    "1. What are the most important hyperparameters of your chosen algorithm?<span style=\"float: right;\">(2 pt)</span>\n",
    "2. Perform hyperparameter optimization (including pre-processing steps)<span style=\"float: right;\">(2 pt)</span>\n",
    "3. Compare at least 3 models with different sets of hyperparameters <span style=\"float: right;\">(1 pt)</span>\n",
    "4. Evaluate the final model (similar to \"Shortlist Promising Models\" above)<span style=\"float: right;\">(5 pt)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e86175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code and markdown cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcdc06e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Enabling Generalization (20 points)\n",
    "Now we are going to train and test a single model that is suitable for operating at all times of the day, i.e., a model that can generalize between times of day and even to unseen times of day, and that is robust to different lighting conditions. The idea is to take the final model you developed above as a starting point and to develop it further for generalization.\n",
    "\n",
    "The simulator provides you access to Time 3. However, this is to be treated as the test set, i.e., you are only allowed to test your model on it as the very final step. I.e., do not tweak your model after running that environment, you would be overfitting to the test data. Performance on the Time 3 environment will NOT influence your grade.\n",
    "\n",
    "\n",
    "1. How can the 3 provided datasets be used to train a model that can generalize, and even more importantly how can they be used to<br> evaluate whether a model can generalize? (Hint: Lecture 2)<span style=\"float: right;\">(2 pt)</span>\n",
    "2. Above you designed features for action classification. Evaluate whether those features are suitable for generalization. If necessary<br> adapt the feature extraction. <span style=\"float: right;\">(2 pt)</span>\n",
    "3. Compare how well models trained just on the data from Time 1 and models trained on data from multiple times of day generalize to<br> unseen times of day (using the approach from the first bullet, do *not* use Time 3 for this comparison). Make sure that this is a fair<br> comparison, e.g., in terms of the amount and quality of the data.<span style=\"float: right;\">(3 pt)</span>\n",
    "4. Discuss at least 2 methods that can be employed to make your model perform better and be robust to the variations we have in this<br> scenario (methods for any step are fine: data collection, data augmentation, pre-processing, model structure, training, etc.)<span style=\"float: right;\">(3 pt)</span>\n",
    "5. Implement at least one of those methods.<span style=\"float: right;\">(1 pt)</span>\n",
    "6. Evaluate the final model (similar to \"Shortlist Promising Models\" above) for generalization making use only of data from Time 0,<br> Time 1, and Time 2.<span style=\"float: right;\">(5 pt)</span>\n",
    "7. Test the final model on the Time 3 environment and discuss its performance.<span style=\"float: right;\">(3 pt)</span>\n",
    "8. Save the parameters of your best multi scenario model to your hard drive (use pickle for sklearn or built-in save/load for keras), you<br> will need to be able to reload your model without training in the next step. Be sure to include the saved parameters in your zip file so<br> we can evaluate your best model too, even without rerunning the notebook up to here.<span style=\"float: right;\">(1 pt)</span>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Remember to provide thorough insights, analysis and reasoning behind your design choices and what you see as a consequence of these choices. Try to synthesize rather than describing what you did step by step.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee30157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code and markdown cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b5093",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Present Your Solution (5 points)\n",
    "1. Summarize your main decisions and insights <span style=\"float: right;\">(2.5 pt)</span>\n",
    "2. Create a stand-alone demo. I.e., a block of cells that can be run on its own. For that you will need to load your pre-trained best<br> model you saved in the previous section and run it on Time 3.<span style=\"float: right;\">(2.5 pt)</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code and markdown cells"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
